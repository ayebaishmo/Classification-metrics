{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4onMpIhLIGdf"
      },
      "source": [
        "BloomTech Data Science\n",
        "\n",
        "*Unit 2, Sprint 2, Module 4*\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-06-13T16:26:05.420861Z",
          "start_time": "2023-06-13T16:26:03.957660Z"
        },
        "id": "gWksJDhpIGdj"
      },
      "outputs": [],
      "source": [
        "# from category_encoders import OrdinalEncoder\n",
        "# from sklearn.ensemble import RandomForestClassifier\n",
        "# from sklearn.impute import SimpleImputer\n",
        "# from sklearn.metrics import plot_confusion_matrix, classification_report\n",
        "# from sklearn.model_selection import train_test_split\n",
        "# from sklearn.pipeline import make_pipeline\n",
        "# import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-06-13T16:26:05.423764Z",
          "start_time": "2023-06-13T16:26:05.421971Z"
        },
        "id": "zdJZDqLKIGdk"
      },
      "outputs": [],
      "source": [
        "# %%capture\n",
        "# import sys\n",
        "\n",
        "# # If you're on Colab:\n",
        "# if 'google.colab' in sys.modules:\n",
        "#     DATA_PATH = 'https://github.com/bloominstituteoftechnology/DS-Unit-2-Kaggle-Challenge/tree/main/data/'\n",
        "#     !pip install category_encoders==2.*\n",
        "#     !pip install pandas-profiling==2.*\n",
        "\n",
        "# # If you're working locally:\n",
        "# else:\n",
        "#     DATA_PATH = '../data/'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nCc3XZEyG3XV"
      },
      "source": [
        "# Module Project: Classification Metrics\n",
        "\n",
        "This sprint, the module projects will focus on creating and improving a model for the Tanazania Water Pump dataset. Your goal is to create a model to predict whether a water pump is functional, non-functional, or needs repair.\n",
        "\n",
        "Dataset source: [DrivenData.org](https://www.drivendata.org/competitions/7/pump-it-up-data-mining-the-water-table/).\n",
        "\n",
        "## Directions\n",
        "\n",
        "The tasks for this project are as follows:\n",
        "\n",
        "- **Task 1:** Use `wrangle` function to import training and test data.\n",
        "- **Task 2:** Split training data into feature matrix `X` and target vector `y`.\n",
        "- **Task 3:** Split training data into training and validation sets.\n",
        "- **Task 4:** Establish the baseline accuracy score for your dataset.\n",
        "- **Task 5:** Build `model`.\n",
        "- **Task 6:** Calculate the training and validation accuracy score for your model.\n",
        "- **Task 7:** Plot the confusion matrix for your model.\n",
        "- **Task 8:** Print the classification report for your model.\n",
        "- **Task 9:** Identify likely `'non-functional'` pumps in the test set.\n",
        "- **Task 10:** Find likely `'non-functional'` pumps serving biggest populations.\n",
        "- **Task 11 (`stretch goal`):** Plot pump locations from Task 10.\n",
        "\n",
        "You should limit yourself to the following libraries for this project:\n",
        "\n",
        "- `category_encoders`\n",
        "- `matplotlib`\n",
        "- `pandas`\n",
        "- `ydata-profiling`\n",
        "- `plotly`\n",
        "- `sklearn`\n",
        "\n",
        "\n",
        "# I. Wrangle Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "kj97F3PdIGdl"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "def wrangle(fm_path, tv_path=None):\n",
        "    if tv_path:\n",
        "        df = pd.merge(pd.read_csv(fm_path,\n",
        "                                  na_values=[0, -2.000000e-08]),\n",
        "                      pd.read_csv(tv_path)).set_index('id')\n",
        "    else:\n",
        "        df = pd.read_csv(fm_path,\n",
        "                         na_values=[0, -2.000000e-08],\n",
        "                         index_col='id')\n",
        "\n",
        "    # Drop constant columns\n",
        "    df.drop(columns=['recorded_by'], inplace=True)\n",
        "\n",
        "    # Drop HCCCs\n",
        "    cutoff = 100\n",
        "    drop_cols = [col for col in df.select_dtypes('object').columns\n",
        "                 if df[col].nunique() > cutoff]\n",
        "    df.drop(columns=drop_cols, inplace=True)\n",
        "\n",
        "    # Drop duplicate columns\n",
        "    dupe_cols = [col for col in df.head(15).T.duplicated().index\n",
        "                 if df.head(15).T.duplicated()[col]]\n",
        "    df.drop(columns=dupe_cols, inplace=True)\n",
        "\n",
        "    return df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uODsQxO3IGdm"
      },
      "source": [
        "**Task 1:** Using the above `wrangle` function to read `train_features.csv` and `train_labels.csv` into the DataFrame `df`, and `test_features.csv` into the DataFrame `X_test`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "UZsdRoZoIGdm"
      },
      "outputs": [],
      "source": [
        "df = wrangle('C:/Users/ISHMO_CT/Downloads/Classification-metrics/Data-sets/train_features.csv',\n",
        "             'C:/Users/ISHMO_CT/Downloads/Classification-metrics/Data-sets/train_labels.csv')\n",
        "\n",
        "X_test = wrangle('C:/Users/ISHMO_CT/Downloads/Classification-metrics/Data-sets/test_features.csv')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sk3nESLsIGdn"
      },
      "source": [
        "# II. Split Data\n",
        "\n",
        "**Task 2:** Split your DataFrame `df` into a feature matrix `X` and the target vector `y`. You want to predict `'status_group'`.\n",
        "\n",
        "**Note:** You won't need to do a train-test split because you'll use cross-validation instead."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "QQdLD-TvIGdn"
      },
      "outputs": [],
      "source": [
        "target = 'status_group'\n",
        "y = df[target]\n",
        "X = df.drop(columns = target)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2REYspV0IGdo"
      },
      "source": [
        "**Task 3:** Using a randomized split, divide `X` and `y` into a training set (`X_train`, `y_train`) and a validation set (`X_val`, `y_val`)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "Ppq46_6uIGdo"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_val, y_train, y_val = train_test_split(X,y, test_size = 0.2, random_state = 42)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WBQVb-WVIGdo"
      },
      "source": [
        "# III. Establish Baseline\n",
        "\n",
        "**Task 4:** Since this is a **classification** problem, you should establish a baseline accuracy score. Figure out what is the majority class in `y_train` and what percentage of your training observations it represents."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "kAk1-HKjIGdo"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Baseline 0.5425489938182296\n"
          ]
        }
      ],
      "source": [
        "print('Baseline', y_train.value_counts(normalize=True).max())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k-4oMBALIGdo"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cIVOdcpLIGdo"
      },
      "source": [
        "Majority Class is Functional. To find percentage of training observations, we would do: percentage = functional / len(y_train). len(y_train) gives me the total number of observations."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1hyMotInIGdp"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XaXig2ORIGdp"
      },
      "source": [
        "# IV. Build Models\n",
        "\n",
        "**Task 5:** Build and train your `model`. Include the transformers and predictor that you think are most appropriate for this problem."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "joo3i-5iIGdp"
      },
      "outputs": [],
      "source": [
        "from category_encoders import OrdinalEncoder\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.pipeline import make_pipeline\n",
        "model = make_pipeline(\n",
        "    OrdinalEncoder(),\n",
        "    SimpleImputer(),\n",
        "    RandomForestClassifier(random_state = 42, n_jobs = -1, )\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OnP7jlkXIGdp"
      },
      "source": [
        "# V. Check Metrics\n",
        "\n",
        "**Task 6:** Calculate the training and validation accuracy scores for `model`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KkfBlRZZIGdp"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nQBPhkosIGdp"
      },
      "source": [
        "**Task 7:** Plot the confusion matrix for your model, using your validation data.\n",
        "\n",
        "**Note:** Since there are three classes in your target vector, the dimensions of your matrix will be 3x3."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LYxLQQolIGdp"
      },
      "outputs": [],
      "source": [
        "# Plot 3x3 confusion matrix\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z8aUCIPYIGdp"
      },
      "source": [
        "Calculating precision and recall for a multiclass problem is a bit of a mess. Fortunately, we can use `sklearn`'s classification report.\n",
        "\n",
        "**Task 8:**  Print the classification report for your `model`, using your validation data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YWDmxXLUIGdp"
      },
      "outputs": [],
      "source": [
        "# Print classification report\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bfpQvCYFIGdq"
      },
      "source": [
        "# VI. Tune Model\n",
        "\n",
        "Usually, we use this part of the ML workflow to adjust the hyperparameters of the our model to increase performance based on metrics like accuracy. Today, we'll use it to help maximize the impact of our water pump repairs when resources are scarce. What if we only had funds to repair 100 water pumps?\n",
        "\n",
        "(This activity is based on a [post](https://towardsdatascience.com/maximizing-scarce-maintenance-resources-with-data-8f3491133050) by Lambda alum Michael Brady.)\n",
        "\n",
        "**Task 9:** Using your model's `predict_proba` method, identify the observations in your **test set** where the model is more than 95% certain that a pump is `'non-functional'`. Put these observations in the DataFrame `X_test_nf`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yoONOufpIGdq"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pM7zZk6BIGdq"
      },
      "source": [
        "**Task 10:** Limit `X_test_nf` to the 100 pumps with the largest associated populations."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mimBmY6aIGdq"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CEBefEybIGdq"
      },
      "source": [
        "# VII. Communicate Results\n",
        "\n",
        "**Task 11 (`stretch goal`):** Create a scatter plot with the location of the 100 pumps in `X_test_nf`.\n",
        "\n",
        "**Note:** If you want to make this a **`super stretch goal`**, create a Mapbox scatter plot using [Plotly](https://plotly.github.io/plotly.py-docs/generated/plotly.express.scatter_mapbox.html)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o_qHSw1vIGdq"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FPfAn8-FIGdq"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A6-NmNuGIGdq"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
